{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76d2ebd5",
   "metadata": {},
   "source": [
    "# Function Description: BinomialLogisticRegression\n",
    "\n",
    "This is a Python function named BinomialLogisticRegression that performs binomial logistic regression on a Pandas DataFrame and predicts the observation's classes based on the model. \n",
    "\n",
    "## Input Parameters\n",
    "\n",
    "- df (pandas.DataFrame): the input dataframe.\n",
    "\n",
    "\n",
    "- opt (bool): a boolean flag to indicate whether or not to optimize the model (default=False).\n",
    "\n",
    "\n",
    "- cutoff (float): a float indicating the threshold for classification (default is 0.5).\n",
    "\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "This function requires the Pandas, NumPy and Scipy libraries to be installed.\n",
    "\n",
    "## Function Steps\n",
    "\n",
    "**1.** The function starts by importing the required libraries, including a custom function for testing the normality of residuals.\n",
    "\n",
    "**2.** It then defines a nested function to check for multicollinearity among the independent variables, excluding columns with perfect correlation to other columns in the DataFrame.\n",
    "\n",
    "**3.** The function calculates the number of observations and variables in the input DataFrame, splitting the predictor and dependent variables, and computing the covariance matrix of the independent variables.\n",
    "\n",
    "**4.** It obtains the coefficients and intercept of the linear regression model by solving the normal equation and prints them.\n",
    "\n",
    "**5.** The function calculates the fitted values and RÂ² of the model, and performs an F-test to check the overall significance of the model. If the F-test is significant, the function then performs a series of T-tests to check the significance of each individual predictor variable.\n",
    "\n",
    "**6.** If the opt parameter of the function is set to True and any of the predictor variables are not significant, the function drops those variables and performs the regression again recursively until all predictor variables are significant or until the maximum number of iterations is reached.\n",
    "\n",
    "**7.** Finally, the function returns a DataFrame with the original input variables and a new column for predicted values based on the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8992b22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BinomialLogisticRegression(df, opt=False, cutoff=.5, it=0):\n",
    "    \n",
    "    '''\n",
    "    The function LogisticRegression performs binomial logistic regression on a Pandas DataFrame and predicts the \n",
    "    observation's classes based on the model. The function requires the Pandas, NumPy, and Scipy libraries installed.\n",
    "    \n",
    "    The function begins by calculating the number of observations and variables in the input DataFrame. It then splits\n",
    "    the predictor and dependent variables and checks whether the dependent variable has two classes. If the dependent\n",
    "    variable does not have two classes, the function prints an error message and returns.\n",
    "    \n",
    "    The multicollinearity_analysis function is defined inside LogisticRegression and aims to exclude columns with\n",
    "    perfect  correlation to other columns in the DataFrame. This function generates a correlation matrix and checks\n",
    "    for columns with a  correlation coefficient of 1. If it finds two such columns, it removes one of them.\n",
    "    \n",
    "    The LogisticRegression function initializes theta and defines the tolerance. It then performs logistic regression\n",
    "    using Newton's method. The predicted classes are obtained by applying a threshold value (default 0.5) to the\n",
    "    probabilities generated by the logistic function. \n",
    "    \n",
    "    The function also performs a series of T-tests to check the significance of each individual predictor variable.\n",
    "    If the \"opt\" parameter of the function is set to True and any of the predictor variables are not significant,\n",
    "    the function drops those variables and performs the regression again recursively until all predictor variables\n",
    "    are significant, to then, initialize theta.\n",
    "    \n",
    "    The he Logistic Regression main metrics, such as the log-likelihood, Akaike information criterion (AIC), \n",
    "    Bayesian information criterion (BIC), Confusion Matrix (based on the predicted classes), Accuracy, Precision,\n",
    "    Recall, Especificity and F1-Score, are also printed. \n",
    "    \n",
    "    Finally, the function returns a DataFrame with the original input variables and a new column for predicted values based\n",
    "    on the logistic regression model.\n",
    "    \n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        df: a Pandas DataFrame, which is the input data for the logistic regression.\n",
    "        opt: a boolean indicating whether or not to optimize the model (default is False).\n",
    "        cutoff: a float indicating the threshold for classification (default is 0.5).\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # Importing needed libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Fuction for Multicollinearity Analysis\n",
    "    def multicollinearity_analysis(df):\n",
    "        \n",
    "        '''This function aims to exclude columns with perfect correlation to other columns in the DataFrame.'''\n",
    "    \n",
    "        # Correlation Matrix\n",
    "        df_cor = df.corr()\n",
    "\n",
    "        # Getting columns with perfect correlation\n",
    "        perfect_correlation_columns = []\n",
    "        for col in df_cor.columns:\n",
    "            for row in df_cor.index:\n",
    "                if col != row:\n",
    "                    if df_cor[col].loc[row] > .98:\n",
    "                        if set([col, row]) not in perfect_correlation_columns:\n",
    "                            perfect_correlation_columns.append(tuple(set([col, row])))\n",
    "                            \n",
    "        # Excluding columns                    \n",
    "        for tuple_col in list(set(perfect_correlation_columns)):\n",
    "            if tuple_col[0] in df:\n",
    "                print(f'The \"{tuple_col[0]}\" column will be deleted for having a perfect correlation with {tuple_col[1]}.\\n')\n",
    "                df = df.drop(tuple_col[0], 1)\n",
    "                      \n",
    "        # Returning treated Dataframe\n",
    "        return df\n",
    "    \n",
    "    # Logistic Function \n",
    "    def logistic(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    # Function for crafting the confusion matrix and related metrics\n",
    "    def confusion_matrix_and_metrics(df):\n",
    "        \n",
    "        '''Develop a confusion matrix based on the predicted and the real values and print the related main metrics.'''\n",
    "        \n",
    "        # Creating the Confusion Matrix\n",
    "        df_classes = df.drop(predictor_columns, 1)\n",
    "        conf_mat = pd.crosstab(index=df_classes[df_classes.columns[1]], columns=df_classes[df_classes.columns[0]])\n",
    "        conf_mat.columns.name = 'Real'\n",
    "        conf_mat.index.name = 'Predicted'\n",
    "        \n",
    "        # Calculating metrics\n",
    "        accuracy = (conf_mat[0].loc[0] + conf_mat[1].loc[1])/conf_mat.sum().sum()\n",
    "        precision = conf_mat[1].loc[1]/conf_mat.sum(1)[1]\n",
    "        recall = conf_mat[1].loc[1]/conf_mat.sum(0)[1]\n",
    "        especificity = conf_mat[0].loc[0]/conf_mat[0].sum()\n",
    "        f1_score = (precision * recall * 2) / (precision + recall)\n",
    "        \n",
    "        # Printing outputs\n",
    "        print(conf_mat)\n",
    "        print(f'\\n\\nAccuracy: {round(accuracy,2)}\\nPrecision: {round(precision,2)}\\nRecall: {round(recall,2)}\\nEspecificity: {round(especificity,2)}\\nF1-score: {round(f1_score,2)}')\n",
    "        \n",
    "        return\n",
    "    \n",
    "    # Check input types\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(\"df must be a pandas DataFrame.\")\n",
    "    if not isinstance(opt, bool):\n",
    "        raise TypeError(\"opt must be a boolean.\")\n",
    "    if not isinstance(it, int):\n",
    "        raise TypeError(\"it must be an integer.\")\n",
    "        \n",
    "    # Checking Null Values\n",
    "    null_vals = df.isna().sum()\n",
    "    if len(null_vals.loc[null_vals>0].index) > 0:\n",
    "        print(f'NaN values in the columns: {null_vals.loc[null_vals>0].index}')\n",
    "        return\n",
    "    \n",
    "    # Excluding columns with perfect correlation\n",
    "    df = multicollinearity_analysis(df)  \n",
    "    \n",
    "    # Checking the dependent variable\n",
    "    y = df.iloc[:,df.shape[1]-1].values\n",
    "    if len(np.unique(y)) != 2:\n",
    "           print('Incorrect number of classes on the dependent variable.')\n",
    "           return\n",
    "    \n",
    "    # Spliting the predictor variables\n",
    "    predictor_columns = df.columns[0:-1]\n",
    "    X = df[predictor_columns].values\n",
    "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    \n",
    "    # Initializing theta and defining tolerance\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    tolerance = 0.0001\n",
    "    \n",
    "    # Iterating for getting the best theta\n",
    "    for i in range(1000):\n",
    "        y_pred = logistic(np.dot(X, theta))    \n",
    "        error = y_pred - y\n",
    "\n",
    "        gradient = X.T @ error\n",
    "        W = np.diag(y_pred * (1 - y_pred))\n",
    "        H = X.T @ W @ X\n",
    "        \n",
    "        new_theta = theta - np.linalg.inv(H) @ gradient\n",
    "        if np.allclose(theta, new_theta, rtol=tolerance):\n",
    "            break\n",
    "        theta = new_theta\n",
    "    \n",
    "    # Getting Logit, p1 and p0\n",
    "    logit = np.dot(X, theta)\n",
    "    p1 = 1 / (1 + np.exp(-logit))\n",
    "    p1 = np.where(p1 == 1, .9999, p1)\n",
    "    p0 = 1 - p1\n",
    "    predictions = np.where(p1 > cutoff, 1, 0)\n",
    "    df['class'] = predictions\n",
    "    \n",
    "    # Calculating log-likelihood, AIC and BIC\n",
    "    ll = sum(y * np.log(p1) + (1 - y)*np.log(p0))\n",
    "    aic = -2 * ll + 2 * (len(predictor_columns) + 1 )\n",
    "    bic = -2 * ll + (len(predictor_columns) + 1) * np.log(len(df))\n",
    "    print(f'Log-likelihood: {round(ll, 4)},\\t AIC: {round(aic, 4)},\\t BIC: {round(bic, 4)}\\n______________________________________________________________________\\n')\n",
    "\n",
    "    # T-student Statistics\n",
    "    W = np.diag(p1*(1-p1))\n",
    "    H = np.dot(X.T, np.dot(W, X))\n",
    "    I = np.linalg.inv(H)\n",
    "    se = np.sqrt(np.diagonal(I))\n",
    "    z = theta / se\n",
    "    p_val = (1 - stats.norm.cdf(abs(z)))*2\n",
    "    \n",
    "    # Initializing the Informative DataFrame\n",
    "    df_info = pd.DataFrame(index=['Intercept'])\n",
    "    \n",
    "    # Intormative DataFrame\n",
    "    df_info = pd.DataFrame(index=['intercept'] + list(predictor_columns), columns = ['Estimate', 'Std. Error', 'Z statistic', 'P value', 'Sig. at 0.05'])\n",
    "    df_info['Estimate'].loc['intercept'] = np.round(theta[0], 4)\n",
    "    df_info['Estimate'][1:] = np.round(theta[1:], 4)\n",
    "    df_info['Std. Error'] = np.round(se,4)\n",
    "    df_info['Z statistic'] = np.round(z,4)\n",
    "    df_info['P value'] = np.round(p_val,4)\n",
    "    df_info['Sig. at 0.05'] = np.where(p_val > 0.05, 'n', 'y')\n",
    "    \n",
    "    # If there's any p-value smaller than 0.05 and the parameter opt == True the function will drop non significant variables\n",
    "    if opt == True and max(df_info['P value'][1:]) > 0.05:\n",
    "        print(df_info)\n",
    "        print('\\n______________________________________________________________________\\nExcluding columns not statistically significants.')\n",
    "        max_index = df_info['P value'].loc[(df_info['P value'] == max(df_info[df_info.index != 'intercept']['P value'])) & (df_info.index != 'intercept')].index[0]\n",
    "        it +=1  \n",
    "        print(f'Excluded column: {max_index}')\n",
    "        df = df.drop(max_index,1)\n",
    "        df = df.drop('class', 1)\n",
    "        print(f'______________________________________________________________________\\n######################################################################')\n",
    "        print(f'\\t\\t\\tIteration number {it}')\n",
    "        print(f'n#####################################################################\\n______________________________________________________________________\\n')\n",
    "        df = BinomialLogisticRegression(df, opt=True, cutoff=cutoff, it=it)\n",
    "        return  df\n",
    "    \n",
    "    # Returning the original Dataframe with the predicted values\n",
    "    else:  \n",
    "        print(df_info)\n",
    "        print('\\n______________________________________________________________________\\n')\n",
    "        df_final = df\n",
    "        print(f'Considering the Cutoff of: {cutoff}\\n\\nConfusion Matrix:\\n')\n",
    "        confusion_matrix_and_metrics(df)\n",
    "        return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
